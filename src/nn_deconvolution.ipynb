{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNhJlOCwOzxR"
      },
      "source": [
        "### **Imports and utils**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi3laPvZ8LaE"
      },
      "source": [
        "Importing libraies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKVUBinurBRv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from astropy.io import fits\n",
        "import numpy as np\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, Concatenate, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFNkwKQL8aah"
      },
      "source": [
        "Utils functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_rUru_Y8bpH"
      },
      "outputs": [],
      "source": [
        "def read_header_and_data(file_path, idx):\n",
        "    with fits.open(file_path) as hdu:\n",
        "        header = hdu[idx].header\n",
        "        data = hdu[idx].data\n",
        "\n",
        "        return header, data\n",
        "\n",
        "def show_image(image_data, cmap='gist_gray'):\n",
        "    plt.figure()\n",
        "    plt.imshow(image_data, cmap=cmap)\n",
        "    plt.colorbar()\n",
        "\n",
        "def mean_subtract(img_data):\n",
        "    img_mean = np.mean(img_data)\n",
        "    img_data -= img_mean\n",
        "\n",
        "    return img_data\n",
        "\n",
        "def normalize(array):\n",
        "    arr_max = np.max(array)\n",
        "    arr_min = np.min(array)\n",
        "\n",
        "    arr_normalized = (array - arr_min) / (arr_max - arr_min)\n",
        "\n",
        "    return arr_normalized\n",
        "\n",
        "def normalize_list(list_arrays):\n",
        "    list_arrays_normalized = []\n",
        "    list_arrays_normalized += [normalize(array) for array in list_arrays]\n",
        "    list_arrays_normalized = np.array(list_arrays_normalized)\n",
        "\n",
        "    return list_arrays_normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-E9JMWnOuNA"
      },
      "source": [
        "### **Denoising**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1V5434SOqwk"
      },
      "source": [
        "Opening denoising data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyzflVO6O8cq"
      },
      "outputs": [],
      "source": [
        "image_file_path = 'outputs/output_denoising_snr100.fits'\n",
        "img_head, img_data = read_header_and_data(image_file_path, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A83RijJAO8rN"
      },
      "source": [
        "ResUNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI8slnVM9MxB"
      },
      "outputs": [],
      "source": [
        "# from paper Qian, H. et al 2022\n",
        "\n",
        "def resnet_block(x, x_shortcut, num_filters, kernel_size=3):\n",
        "    # Main block\n",
        "    rb = Conv2D(num_filters, kernel_size, padding='same')(x)\n",
        "    rb = BatchNormalization()(rb)\n",
        "    rb = Activation('relu')(rb)\n",
        "    rb = Conv2D(num_filters, kernel_size, padding='same')(rb)\n",
        "\n",
        "    # Add\n",
        "    x_shortcut = Conv2D(num_filters, kernel_size=1, padding='same')(x_shortcut)\n",
        "    #x_shortcut = BatchNormalization()(x_shortcut)\n",
        "    rb = Add()([x_shortcut, rb])\n",
        "\n",
        "    return rb\n",
        "\n",
        "def upsample_concatenate(x, skip):\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "\n",
        "    return Concatenate()([x, skip])\n",
        "\n",
        "def resUnet(input_shape=(48, 48, 1), num_filters=64, num_resnetblocks=4, kernel_size=3): # change filters dynamically and check blocks\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = inputs\n",
        "    skip_connections = []\n",
        "    for i in range(num_resnetblocks):\n",
        "        if not i == 0: x = BatchNormalization()(x)\n",
        "        x = resnet_block(x, x, num_filters, kernel_size)\n",
        "        if not i == num_resnetblocks-1:\n",
        "            skip_connections += [x] # copy maybe\n",
        "            x = Conv2D(num_filters, kernel_size, strides=2, padding='same')(x)\n",
        "\n",
        "    # Transition\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_filters * 2, kernel_size, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_filters * 2, kernel_size, padding='same')(x)\n",
        "\n",
        "    # Decoder\n",
        "    for skip in reversed(skip_connections):\n",
        "        x = upsample_concatenate(x, skip)\n",
        "        x_shortcut = x\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = resnet_block(x, x_shortcut, num_filters, kernel_size)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(num_filters, kernel_size, padding='same')(x)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    network = Model(inputs, outputs)\n",
        "\n",
        "    return network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb2yzuXt9MxB"
      },
      "source": [
        "Training ResUnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "7qsNlLX99MxB",
        "outputId": "e7b62d8d-6ef8-4451-f663-075a149ed4d0"
      },
      "outputs": [],
      "source": [
        "#img_data = np.random.permutation(img_data)\n",
        "img_data = normalize_list(img_data)\n",
        "img_size = img_data[0].shape[0]\n",
        "num_total = len(img_data)\n",
        "train_split = 0.7\n",
        "train_idx = int(num_total * train_split)\n",
        "\n",
        "img_data = np.array([np.reshape(img, (img_size, img_size, 1)) for img in img_data])\n",
        "img_shape = img_data[0].shape\n",
        "\n",
        "# Spliting training and testing\n",
        "train = img_data[0:train_idx]\n",
        "train_x = train[[i for i in range(0, len(train), 2)]]\n",
        "train_y = train[[i for i in range(1, len(train), 2)]]\n",
        "\n",
        "test = img_data[train_idx:num_total]\n",
        "test_x = test[[i for i in range(0, len(test), 2)]]\n",
        "test_y = test[[i for i in range(1, len(test), 2)]]\n",
        "\n",
        "idx = 0\n",
        "show_image(test_x[idx])\n",
        "show_image(test_y[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzMv-7xO9MxD",
        "outputId": "1eabf1f2-3528-4802-ff6c-6b7061bbfd70"
      },
      "outputs": [],
      "source": [
        "resunet_model = resUnet(input_shape=img_shape, num_filters=64, num_resnetblocks=4, kernel_size=3)\n",
        "resunet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtzvV7pSpcOO",
        "outputId": "87e62528-3fd6-4ac6-8765-6dc791744970"
      },
      "outputs": [],
      "source": [
        "resunet_model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 25\n",
        "history = resunet_model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(test_x, test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "jm9QmL-KgDtF",
        "outputId": "0c8eba31-289f-4175-d91a-acb8361808da"
      },
      "outputs": [],
      "source": [
        "predictions = resunet_model.predict(test_x)\n",
        "idx = 0\n",
        "show_image(test_x[idx])\n",
        "show_image(predictions[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXteIN4RN_m8"
      },
      "source": [
        "Saving model to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnSLFlNFOBeG"
      },
      "outputs": [],
      "source": [
        "resunet_model.save('resunet_100snr2.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZf-h-Mb6kma"
      },
      "source": [
        "**Deconvolution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing images and spliting training and testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing images\n",
        "image_file_path = 'outputs/output_deconv_snr100_5k.fits'\n",
        "img_head, img_data = read_header_and_data(image_file_path, 0) # img_data = [mod_img1, original_img1, mod_img2, original_img2, ...]\n",
        "\n",
        "image_psf_file_path = 'outputs/output_deconv_snr100_5k_psf.fits'\n",
        "psf_head, psf_data = read_header_and_data(image_psf_file_path, 0)\n",
        "\n",
        "# Getting image size\n",
        "img_size = img_data[0].shape[0]\n",
        "\n",
        "# Normalizing images\n",
        "img_data = normalize_list(img_data)\n",
        "psf_data = normalize_list(psf_data)\n",
        "\n",
        "# Reshaping images\n",
        "img_data = np.array([np.reshape(img, (img_size, img_size, 1)) for img in img_data])\n",
        "psf_data = np.array([np.reshape(psf, (img_size, img_size, 1)) for psf in psf_data])\n",
        "img_shape = img_data[0].shape\n",
        "\n",
        "# Spliting training and testing\n",
        "train_split = 0.7\n",
        "img_size = img_data[0].shape[0]\n",
        "num_total = len(img_data)\n",
        "train_idx = int(num_total * train_split)\n",
        "\n",
        "img_train = img_data[0:train_idx]\n",
        "psf_train = psf_data[0:train_idx]\n",
        "\n",
        "img_test = img_data[train_idx:num_total]\n",
        "psf_test = psf_data[train_idx:num_total]\n",
        "\n",
        "# Image train\n",
        "img_train_x = img_train[[i for i in range(0, len(img_train), 2)]]\n",
        "img_train_y = img_train[[i for i in range(1, len(img_train), 2)]]\n",
        "\n",
        "# PSF of the train images\n",
        "psf_train_x = psf_train[[i for i in range(0, len(psf_train), 2)]]\n",
        "\n",
        "# Image test\n",
        "img_test_x = img_test[[i for i in range(0, len(img_test), 2)]]\n",
        "img_test_y = img_test[[i for i in range(1, len(img_test), 2)]]\n",
        "\n",
        "# PSF of the test images\n",
        "psf_test_x = psf_test[[i for i in range(0, len(psf_test), 2)]]\n",
        "\n",
        "idx = 0\n",
        "show_image(img_train_x[idx])\n",
        "show_image(psf_train_x[idx])\n",
        "show_image(img_train_y[idx])\n",
        "show_image(img_test_x[idx])\n",
        "show_image(psf_test_x[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix1wWjWi66SY"
      },
      "source": [
        "Unrolled ADMM Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUR5RuTp65mW"
      },
      "outputs": [],
      "source": [
        "def compute_alpha(N, y, beta=0.33):\n",
        "    return np.sum(y) / (N*beta)\n",
        "\n",
        "def unrolled_admm(input_shape, n):\n",
        "    # Inputs\n",
        "    y_input = Input(shape=input_shape)\n",
        "    h_input = Input(shape=input_shape)\n",
        "\n",
        "    # Get inputs as np objects\n",
        "    y = tf.keras.backend.eval(y_input) \n",
        "    h = tf.keras.backend.eval(h_input)\n",
        "    \n",
        "    # Initializing parameters\n",
        "    N = input_shape[0]\n",
        "    alpha = compute_alpha(N, y) # 500, 200\n",
        "    alpha_inv = 1 / alpha\n",
        "\n",
        "    # Initializing denoising network\n",
        "    resunet = resUnet(input_shape, num_filters=64, num_resnetblocks=4, kernel_size=3)\n",
        "\n",
        "    # Initialize x with Wiener Filter\n",
        "    Y = np.fft.fft(y)\n",
        "    H = np.fft.fft(h)\n",
        "    Ht = H.T\n",
        "    Ht_conj = Ht.conj()\n",
        "    H_abs_sqr = np.abs(H)**2 # np.linalg.det(H)**2\n",
        "    x = [alpha_inv * np.fft.ifft((Ht_conj * Y) / (alpha_inv + H_abs_sqr))]\n",
        "\n",
        "    # Initialize other parameters\n",
        "    z = [x[0]]\n",
        "    v = [np.copy(y)]\n",
        "    u1 = [0]\n",
        "    u2 = [0]\n",
        "\n",
        "    # Tilde variables are calculated inside main loop\n",
        "    x0_tilde = [] \n",
        "    x1_tilde = []\n",
        "    v_tilde = [] \n",
        "    z_tilde = []\n",
        "\n",
        "    # Hyperparameters (can be initialized by nn)\n",
        "    rho1 = 1 # 10e3 # rho1 in (10−5, 10−2)\n",
        "    rho2 = 1 # rho2 in ?\n",
        "    # gamma = 1.5 # gamma in (1, 2)\n",
        "    # eta = 0.5 # ?\n",
        "\n",
        "    # ADMM iterations\n",
        "    for k in range(1, n): # 1..n?\n",
        "        v_tilde[k-1] = np.convolve(h, x[k-1]) + u2[k-1]\n",
        "        v[k] = (rho2 * v_tilde[k-1] + y) / (1 + rho2) # gaussian mle\n",
        "\n",
        "        z_tilde[k-1] = x[k-1] + u1[k-1]\n",
        "        z[k] = tf.keras.backend.eval(resunet(z_tilde[k-1])) # eq 13\n",
        "\n",
        "        x0_tilde[k-1] = z[k] - u1[k-1]\n",
        "        x1_tilde[k-1] = v[k] - u2[k-1]\n",
        "        X0_tilde = np.fft.fft(x0_tilde[k-1])\n",
        "        X1_tilde = np.fft.fft(x1_tilde[k-1])\n",
        "        rho_ratio = rho2 / rho1\n",
        "        x[k] = np.fft.ifft((X0_tilde + rho_ratio * Ht_conj * X1_tilde) / (1 + rho_ratio * H_abs_sqr)) # eq 12\n",
        "\n",
        "        # update Lagrangian multipliers\n",
        "        u1[k] = u1[k-1] + x[k] - z[k] \n",
        "        u2[k] = u2[k-1] + np.convolve(H, x[k]) - v[k]\n",
        "\n",
        "    # Convert processed image back to tensor\n",
        "    output_img = tf.convert_to_tensor(np.real(x[-1]), dtype=tf.float32) # how to connect\n",
        "\n",
        "    output = Conv2D(1, 1, activation='sigmoid', padding='same')(output_img)\n",
        "    network = Model(inputs=[y, h], outputs=output)\n",
        "\n",
        "    return network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unrolled_admm_model = unrolled_admm(input_shape=(48, 48, 1), n=8)\n",
        "unrolled_admm_model.compile(optimizer=Adam(), loss='mean_absolute_error', metrics=['mae'])\n",
        "\n",
        "batch_size = None # 128\n",
        "epochs = 25\n",
        "\n",
        "history = unrolled_admm_model.fit([img_train_x, psf_train_x], img_train_y, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=([img_test_x, psf_test_x], img_test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating model as subclass\n",
        "\n",
        "class UnrolledADMM(Model):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.n = n\n",
        "    \n",
        "    def compute_alpha(self, N, y, beta=0.33):\n",
        "        print(\"calc alpha\")\n",
        "        return np.sum(y) / (N*beta)\n",
        "    \n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        y = inputs[0]\n",
        "        h = inputs[1]       \n",
        "\n",
        "        # Initializing parameters\n",
        "        N = tf.shape(y)\n",
        "        print(N)\n",
        "        alpha = self.compute_alpha(N, y) # 500, 200\n",
        "        print(\"done calc alpha\")\n",
        "        alpha_inv = 1 / alpha\n",
        "\n",
        "        # Initializing denoising network\n",
        "        resunet = resUnet(input_shape=(N, N, 1), num_filters=64, num_resnetblocks=4, kernel_size=3)\n",
        "\n",
        "        # Initialize x with Wiener Filter\n",
        "        Y = np.fft.fft(y)\n",
        "        H = np.fft.fft(h)\n",
        "        Ht = H.T\n",
        "        Ht_conj = Ht.conj()\n",
        "        H_abs_sqr = np.abs(H)**2 # np.linalg.det(H)**2\n",
        "        x = [alpha_inv * np.fft.ifft((Ht_conj * Y) / (alpha_inv + H_abs_sqr))]\n",
        "\n",
        "        # Initialize other parameters\n",
        "        z = [x[0]]\n",
        "        v = [np.copy(y)]\n",
        "        u1 = [0]\n",
        "        u2 = [0]\n",
        "\n",
        "        # Tilde variables are calculated inside main loop\n",
        "        x0_tilde = [] \n",
        "        x1_tilde = []\n",
        "        v_tilde = [] \n",
        "        z_tilde = []\n",
        "\n",
        "        # Hyperparameters (can be initialized by nn)\n",
        "        rho1 = 1 # 10e3 # rho1 in (10−5, 10−2)\n",
        "        rho2 = 1 # rho2 in ?\n",
        "        # gamma = 1.5 # gamma in (1, 2)\n",
        "        # eta = 0.5 # ?\n",
        "\n",
        "        # ADMM iterations\n",
        "        for k in range(1, self.n): # 1..n?\n",
        "            v_tilde[k-1] = np.convolve(h, x[k-1]) + u2[k-1]\n",
        "            v[k] = (rho2 * v_tilde[k-1] + y) / (1 + rho2) # gaussian mle\n",
        "\n",
        "            z_tilde[k-1] = x[k-1] + u1[k-1]\n",
        "            z[k] = tf.keras.backend.eval(resunet(z_tilde[k-1])) # eq 13\n",
        "\n",
        "            x0_tilde[k-1] = z[k] - u1[k-1]\n",
        "            x1_tilde[k-1] = v[k] - u2[k-1]\n",
        "            X0_tilde = np.fft.fft(x0_tilde[k-1])\n",
        "            X1_tilde = np.fft.fft(x1_tilde[k-1])\n",
        "            rho_ratio = rho2 / rho1\n",
        "            x[k] = np.fft.ifft((X0_tilde + rho_ratio * Ht_conj * X1_tilde) / (1 + rho_ratio * H_abs_sqr)) # eq 12\n",
        "\n",
        "            # update Lagrangian multipliers\n",
        "            u1[k] = u1[k-1] + x[k] - z[k] \n",
        "            u2[k] = u2[k-1] + np.convolve(H, x[k]) - v[k]\n",
        "\n",
        "        return x[-1] # return last image of iterations\n",
        "        \n",
        "        #return super().call(inputs, training, mask)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unrolled_admm_model = UnrolledADMM(n=8)\n",
        "unrolled_admm_model.compile(optimizer=Adam(), loss='mean_absolute_error', metrics=['mae'])\n",
        "\n",
        "batch_size = None # 128\n",
        "epochs = 25\n",
        "\n",
        "history = unrolled_admm_model.fit([img_train_x, psf_train_x], img_train_y, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=([img_test_x, psf_test_x], img_test_y))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
